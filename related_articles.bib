@inproceedings{DeYoung:2020,
    title = {{ERASER}: {A} Benchmark to Evaluate Rationalized {NLP} Models},
    author = {DeYoung, Jay  and
      Jain, Sarthak  and
      Rajani, Nazneen Fatema  and
      Lehman, Eric  and
      Xiong, Caiming  and
      Socher, Richard  and
      Wallace, Byron C.},
    booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
    month = {jul},
    year = {2020},
    address = {Online},
    publisher = {Association for Computational Linguistics},
    url = {https://aclanthology.org/2020.acl-main.408},
    doi = {10.18653/v1/2020.acl-main.408},
    pages = {4443--4458},
    abstract = {State-of-the-art models in NLP are now predominantly based on deep neural networks that are opaque in terms of how they come to make predictions. This limitation has increased interest in designing more interpretable deep models for NLP that reveal the {`}reasoning{'} behind model outputs. But work in this direction has been conducted on different datasets and tasks with correspondingly unique aims and metrics; this makes it difficult to track progress. We propose the \textbf{E}valuating \textbf{R}ationales \textbf{A}nd \textbf{S}imple \textbf{E}nglish \textbf{R}easoning (\textbf{ERASER} a benchmark to advance research on interpretable models in NLP. This benchmark comprises multiple datasets and tasks for which human annotations of {``}rationales{''} (supporting evidence) have been collected. We propose several metrics that aim to capture how well the rationales provided by models align with human rationales, and also how \textit{faithful} these rationales are (i.e., the degree to which provided rationales influenced the corresponding predictions). Our hope is that releasing this benchmark facilitates progress on designing more interpretable NLP systems. The benchmark, code, and documentation are available at \url{https://www.eraserbenchmark.com/}},
}

@inproceedings{Mathew:2021,
	title={HateXplain: A Benchmark Dataset for Explainable Hate Speech Detection},
	author={Mathew, Binny and Saha, Punyajoy and Yimam, Seid Muhie and Biemann, Chris and Goyal, Pawan and Mukherjee, Animesh},
	booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
	volume={35},
	pages={14867--14875},
	year={2021},
	url={https://ojs.aaai.org/index.php/AAAI/article/download/17745/17552}
}


@inproceedings{Thorne:2018,
	title = {{FEVER}: a Large-scale Dataset for Fact Extraction and {VER}ification},
	author = {Thorne, James  and
	Vlachos, Andreas  and
	Christodoulopoulos, Christos  and
	Mittal, Arpit},
	booktitle = {Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)},
	month = {jun},
	year = {2018},
	address = {New Orleans, Louisiana},
	publisher = {Association for Computational Linguistics},
	url = {https://aclanthology.org/N18-1074},
	doi = {10.18653/v1/N18-1074},
	pages = {809--819},
	abstract = {In this paper we introduce a new publicly available dataset for verification against textual sources, FEVER: Fact Extraction and VERification. It consists of 185,445 claims generated by altering sentences extracted from Wikipedia and subsequently verified without knowledge of the sentence they were derived from. The claims are classified as Supported, Refuted or NotEnoughInfo by annotators achieving 0.6841 in Fleiss kappa. For the first two classes, the annotators also recorded the sentence(s) forming the necessary evidence for their judgment. To characterize the challenge of the dataset presented, we develop a pipeline approach and compare it to suitably designed oracles. The best accuracy we achieve on labeling a claim accompanied by the correct evidence is 31.87{\%}, while if we ignore the evidence we achieve 50.91{\%}. Thus we believe that FEVER is a challenging testbed that will help stimulate progress on claim verification against textual sources.},
}

@inproceedings{Camburu:2018,
	title = {e-{SNLI}: Natural Language Inference with Natural Language Explanations.},
	author = {Camburu, Oana-Maria and Rocktäschel, Tim and Lukasiewicz, Thomas and Blunsom, Phil},
	biburl = {https://www.bibsonomy.org/bibtex/2b3a692447484edc53a53a1d94223e1c2/dblp},
	booktitle = {NeurIPS},
	editor = {Bengio, Samy and Wallach, Hanna M. and Larochelle, Hugo and Grauman, Kristen and Cesa-Bianchi, Nicolò and Garnett, Roman},
	ee = {http://papers.nips.cc/paper/8163-e-snli-natural-language-inference-with-natural-language-explanations},
	interhash = {bd00136982e53474b025350d5d6a64bb},
	intrahash = {b3a692447484edc53a53a1d94223e1c2},
	keywords = {dblp},
	pages = {9560-9572},
	timestamp = {2020-03-07T11:50:35.000+0100},
	url = {http://dblp.uni-trier.de/db/conf/nips/nips2018.html#CamburuRLB18},
	year = 2018
}

@inproceedings{DeYoung:2020b,
	title = {Evidence Inference 2.0: More Data, Better Models"},
	author = {DeYoung, Jay  and
	Lehman, Eric  and
	Nye, Benjamin  and
	Marshall, Iain  and
	Wallace, Byron C.},
	booktitle = {Proceedings of the 19th SIGBioMed Workshop on Biomedical Language Processing},
	month = {jul},
	year = {2020},
	address = {Online},
	publisher = {Association for Computational Linguistics},
	url = {https://www.aclweb.org/anthology/2020.bionlp-1.13},
	pages = {123--132},
	abstract = {How do we most effectively treat a disease or condition? Ideally, we could consult a database of evidence gleaned from clinical trials to answer such questions. Unfortunately, no such database exists; clinical trial results are instead disseminated primarily via lengthy natural language articles. Perusing all such articles would be prohibitively time-consuming for healthcare practitioners; they instead tend to depend on manually compiled \textit{systematic reviews} of medical literature to inform care. NLP may speed this process up, and eventually facilitate immediate consult of published evidence. The \textit{Evidence Inference} dataset was recently released to facilitate research toward this end. This task entails inferring the comparative performance of two treatments, with respect to a given outcome, from a particular article (describing a clinical trial) and identifying supporting evidence. For instance: Does this article report that \textit{chemotherapy} performed better than \textit{surgery} for \textit{five-year survival rates} of operable cancers? In this paper, we collect additional annotations to expand the Evidence Inference dataset by 25{\%}, provide stronger baseline models, systematically inspect the errors that these make, and probe dataset quality. We also release an \textit{abstract only} (as opposed to full-texts) version of the task for rapid model prototyping. The updated corpus, documentation, and code for new baselines and evaluations are available at \url{http://evidence-inference.ebm-nlp.com/}.},
}

@inproceedings{Clark:2019,
	title = {{B}ool{Q}: Exploring the Surprising Difficulty of Natural Yes/No Questions},
	author = {Clark, Christopher  and
	Lee, Kenton  and
	Chang, Ming-Wei  and
	Kwiatkowski, Tom  and
	Collins, Michael  and
	Toutanova, Kristina},
	booktitle = {Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
	month = {jun},
	year = {2019},
	address = {Minneapolis, Minnesota},
	publisher = {Association for Computational Linguistics},
	url = {https://aclanthology.org/N19-1300},
	doi = {10.18653/v1/N19-1300},
	pages = {2924--2936},
	abstract = {In this paper we study yes/no questions that are naturally occurring {---} meaning that they are generated in unprompted and unconstrained settings. We build a reading comprehension dataset, BoolQ, of such questions, and show that they are unexpectedly challenging. They often query for complex, non-factoid information, and require difficult entailment-like inference to solve. We also explore the effectiveness of a range of transfer learning baselines. We find that transferring from entailment data is more effective than transferring from paraphrase or extractive QA data, and that it, surprisingly, continues to be very beneficial even when starting from massive pre-trained language models such as BERT. Our best method trains BERT on MultiNLI and then re-trains it on our train set. It achieves 80.4{\%} accuracy compared to 90{\%} accuracy of human annotators (and 62{\%} majority-baseline), leaving a significant gap for future work.},
}

@inproceedings{Rajani:2019,
	title = {Explain Yourself! {L}everaging Language Models for Commonsense Reasoning},
	author = {Rajani, Nazneen Fatema  and
	McCann, Bryan  and
	Xiong, Caiming  and
	Socher, Richard},
	booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
	month = {jul},
	year = {2019},
	address = {Florence, Italy},
	publisher = {Association for Computational Linguistics},
	url = {https://aclanthology.org/P19-1487},
	doi = {10.18653/v1/P19-1487},
	pages = {4932--4942},
	abstract = {Deep learning models perform poorly on tasks that require commonsense reasoning, which often necessitates some form of world-knowledge or reasoning over information not immediately present in the input. We collect human explanations for commonsense reasoning in the form of natural language sequences and highlighted annotations in a new dataset called Common Sense Explanations (CoS-E). We use CoS-E to train language models to automatically generate explanations that can be used during training and inference in a novel Commonsense Auto-Generated Explanation (CAGE) framework. CAGE improves the state-of-the-art by 10{\%} on the challenging CommonsenseQA task. We further study commonsense reasoning in DNNs using both human and auto-generated explanations including transfer to out-of-domain tasks. Empirical results indicate that we can effectively leverage language models for commonsense reasoning.},
}

@inproceedings{Zaidan:2007,
	title = "Using {``}Annotator Rationales{''} to Improve Machine Learning for Text Categorization",
	author = "Zaidan, Omar  and
	Eisner, Jason  and
	Piatko, Christine",
	booktitle = "Human Language Technologies 2007: The Conference of the North {A}merican Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference",
	month = apr,
	year = "2007",
	address = "Rochester, New York",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/N07-1033",
	pages = "260--267",
}


@InProceedings{Zaidan:2008,
	author    =  {Omar F. Zaidan  and  Jason Eisner  and  Christine Piatko},
	title     =  {Machine Learning with Annotator Rationales to Reduce Annotation Cost},
	booktitle =  {Proceedings of the NIPS*2008 Workshop on Cost Sensitive Learning},
	month     =  {December},
	year      =  {2008},
	url       =  {https://www.cs.jhu.edu/~jason/papers/zaidan+al.nipsw08.pdf}
}

@inproceedings{Khashabi:2018,
	title = {Looking Beyond the Surface: A Challenge Set for Reading Comprehension over Multiple Sentences},
	author = {Khashabi, Daniel  and
	Chaturvedi, Snigdha  and
	Roth, Michael  and
	Upadhyay, Shyam  and
	Roth, Dan},
	booktitle = {Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)},
	month = {jun},
	year = {2018},
	address = {New Orleans, Louisiana},
	publisher = {Association for Computational Linguistics},
	url = {https://aclanthology.org/N18-1023},
	doi = {10.18653/v1/N18-1023},
	pages = {252--262},
	abstract = {We present a reading comprehension challenge in which questions can only be answered by taking into account information from multiple sentences. We solicit and verify questions and answers for this challenge through a 4-step crowdsourcing experiment. Our challenge dataset contains 6,500+ questions for 1000+ paragraphs across 7 different domains (elementary school science, news, travel guides, fiction stories, etc) bringing in linguistic diversity to the texts and to the questions wordings. On a subset of our dataset, we found human solvers to achieve an F1-score of 88.1{\%}. We analyze a range of baselines, including a recent state-of-art reading comprehension system, and demonstrate the difficulty of this challenge, despite a high human performance. The dataset is the first to study multi-sentence inference at scale, with an open-ended set of question types that requires reasoning skills.},
}

@inproceedings{Wadden:2020,
	title = {Fact or Fiction: Verifying Scientific Claims},
	author = {Wadden, David  and
	Lin, Shanchuan  and
	Lo, Kyle  and
	Wang, Lucy Lu  and
	van Zuylen, Madeleine  and
	Cohan, Arman  and
	Hajishirzi, Hannaneh},
	booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
	month = {nov},
	year = {2020},
	address = {Online},
	publisher = {Association for Computational Linguistics},
	url = {https://aclanthology.org/2020.emnlp-main.609},
	doi = {10.18653/v1/2020.emnlp-main.609},
	pages = {7534--7550},
	abstract = {We introduce scientific claim verification, a new task to select abstracts from the research literature containing evidence that SUPPORTS or REFUTES a given scientific claim, and to identify rationales justifying each decision. To study this task, we construct SciFact, a dataset of 1.4K expert-written scientific claims paired with evidence-containing abstracts annotated with labels and rationales. We develop baseline models for SciFact, and demonstrate that simple domain adaptation techniques substantially improve performance compared to models trained on Wikipedia or political news. We show that our system is able to verify claims related to COVID-19 by identifying evidence from the CORD-19 corpus. Our experiments indicate that SciFact will provide a challenging testbed for the development of new systems designed to retrieve and reason over corpora containing specialized domain knowledge. Data and code for this new task are publicly available at https://github.com/allenai/scifact. A leaderboard and COVID-19 fact-checking demo are available at https://scifact.apps.allenai.org.},
}

@inproceedings{Carton:2018,
	title = {Extractive Adversarial Networks: {H}igh-Recall Explanations for Identifying Personal Attacks in Social Media Posts},
	author = {Carton, Samuel  and
	Mei, Qiaozhu  and
	Resnick, Paul},
	booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
	month = oct # "-" # nov,
	year = {2018},
	address = {Brussels, Belgium},
	publisher = {Association for Computational Linguistics},
	url = {https://aclanthology.org/D18-1386},
	doi = {10.18653/v1/D18-1386},
	pages = {3497--3507},
	abstract = {We introduce an adversarial method for producing high-recall explanations of neural text classifier decisions. Building on an existing architecture for extractive explanations via hard attention, we add an adversarial layer which scans the residual of the attention for remaining predictive signal. Motivated by the important domain of detecting personal attacks in social media comments, we additionally demonstrate the importance of manually setting a semantically appropriate {``}default{''} behavior for the model by explicitly manipulating its bias term. We develop a validation set of human-annotated personal attacks to evaluate the impact of these changes.},
}

@inproceedings{Socher:2013,
	title = {Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank},
	author = {Socher, Richard  and
	Perelygin, Alex  and
	Wu, Jean  and
	Chuang, Jason  and
	Manning, Christopher D.  and
	Ng, Andrew  and
	Potts, Christopher},
	booktitle = {Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing},
	month = oct,
	year = {2013},
	address = {Seattle, Washington, USA},
	publisher = {Association for Computational Linguistics},
	url = {https://aclanthology.org/D13-1170},
	pages = {1631--1642},
}

@misc{McAuley:2012,
	doi = {10.48550/ARXIV.1210.3926},
	url = {https://arxiv.org/abs/1210.3926},
	author = {McAuley, Julian and Leskovec, Jure and Jurafsky, Dan},
	keywords = {Computation and Language (cs.CL), Information Retrieval (cs.IR), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
	title = {Learning Attitudes and Attributes from Multi-Aspect Reviews},
	publisher = {arXiv},
	year = {2012},
	copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{Wiegreffe:2021,
	author = {Wiegreffe, Sarah and Marasovic, Ana},
	booktitle = {Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks},
	editor = {J. Vanschoren and S. Yeung},
	pages = {},
	title = {Teach Me to Explain: A Review of Datasets for Explainable Natural Language Processing},
	url = {https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/file/698d51a19d8a121ce581499d7b701668-Paper-round1.pdf},
	volume = {1},
	year = {2021}
}



@inproceedings{Wang:2019,
	title = {Does it Make Sense? {A}nd Why? {A} Pilot Study for Sense Making and Explanation},
	author = {Wang, Cunxiang  and
	Liang, Shuailong  and
	Zhang, Yue  and
	Li, Xiaonan  and
	Gao, Tian},
	booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
	month = {jul},
	year = {2019},
	address = {Florence, Italy},
	publisher = {Association for Computational Linguistics},
	url = {https://aclanthology.org/P19-1393},
	doi = {10.18653/v1/P19-1393},
	pages = {4020--4026},
	abstract = {Introducing common sense to natural language understanding systems has received increasing research attention. It remains a fundamental question on how to evaluate whether a system has the sense-making capability. Existing benchmarks measure common sense knowledge indirectly or without reasoning. In this paper, we release a benchmark to directly test whether a system can differentiate natural language statements that make sense from those that do not make sense. In addition, a system is asked to identify the most crucial reason why a statement does not make sense. We evaluate models trained over large-scale language modeling tasks as well as human performance, showing that there are different challenges for system sense-making.},
}



@inproceedings{Jain:2020,
	title = {{L}earning to Faithfully Rationalize by Construction},
	author = {Jain, Sarthak  and
	Wiegreffe, Sarah  and
	Pinter, Yuval  and
	Wallace, Byron C.},
	booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
	month = {jul},
	year = {2020},
	address = {Online},
	publisher = {Association for Computational Linguistics},
	url = {https://aclanthology.org/2020.acl-main.409},
	doi = {10.18653/v1/2020.acl-main.409},
	pages = {4459--4473},
	abstract = {In many settings it is important for one to be able to understand why a model made a particular prediction. In NLP this often entails extracting snippets of an input text {`}responsible for{'} corresponding model output; when such a snippet comprises tokens that indeed informed the model{'}s prediction, it is a faithful explanation. In some settings, faithfulness may be critical to ensure transparency. Lei et al. (2016) proposed a model to produce faithful rationales for neural text classification by defining independent snippet extraction and prediction modules. However, the discrete selection over input tokens performed by this method complicates training, leading to high variance and requiring careful hyperparameter tuning. We propose a simpler variant of this approach that provides faithful explanations by construction. In our scheme, named FRESH, arbitrary feature importance scores (e.g., gradients from a trained model) are used to induce binary labels over token inputs, which an extractor can be trained to predict. An independent classifier module is then trained exclusively on snippets provided by the extractor; these snippets thus constitute faithful explanations, even if the classifier is arbitrarily complex. In both automatic and manual evaluations we find that variants of this simple framework yield predictive performance superior to {`}end-to-end{'} approaches, while being more general and easier to train. Code is available at https://github.com/successar/FRESH.},
}


@inproceedings{Han:2020,
    title = {Explaining Black Box Predictions and Unveiling Data Artifacts through Influence Functions},
    author = {Han, Xiaochuang  and
      Wallace, Byron C.  and
      Tsvetkov, Yulia},
    booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
    month = {jul},
    year = {2020},
    address = {Online},
    publisher = {Association for Computational Linguistics},
    url = {https://aclanthology.org/2020.acl-main.492},
    doi = {10.18653/v1/2020.acl-main.492},
    pages = {5553--5563},
    abstract = {Modern deep learning models for NLP are notoriously opaque. This has motivated the development of methods for interpreting such models, e.g., via gradient-based saliency maps or the visualization of attention weights. Such approaches aim to provide explanations for a particular model prediction by highlighting important words in the corresponding input text. While this might be useful for tasks where decisions are explicitly influenced by individual tokens in the input, we suspect that such highlighting is not suitable for tasks where model decisions should be driven by more complex reasoning. In this work, we investigate the use of influence functions for NLP, providing an alternative approach to interpreting neural text classifiers. Influence functions explain the decisions of a model by identifying influential training examples. Despite the promise of this approach, influence functions have not yet been extensively evaluated in the context of NLP, a gap addressed by this work. We conduct a comparison between influence functions and common word-saliency methods on representative tasks. As suspected, we find that influence functions are particularly useful for natural language inference, a task in which {`}saliency maps{'} may not have clear interpretation. Furthermore, we develop a new quantitative measure based on influence functions that can reveal artifacts in training data.},
}

@InProceedings{Ruiter:2022,
  author    = {Ruiter, Dana  and  Reiners, Liane and Geet D'Sa, Ashwin and Kleinbauer, Thomas and Fohr, Dominique and Illina, Irina and Klakow, Dietrich and Schemer, Christian and Monnier, Angeliki},
  title     = {Placing M-Phasis on the Plurality of Hate: A Feature-Based Corpus of Hate Online},
  booktitle      = {Proceedings of The 14th Language Resources and Evaluation Conference},
  month          = {June},
  year           = {2022},
  address        = {Marseille, France},
  publisher      = {European Language Resources Association},
  url            = {https://arxiv.org/pdf/2204.13400.pdf}
}

@inproceedings{Zhang:2021,
  author = {Zhang, Zijian and Rudra, Koustav and Anand, Avishek},
  title = {Explain and Predict, and Then Predict Again},
  year = {2021},
  isbn = {9781450382977},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3437963.3441758},
  doi = {10.1145/3437963.3441758},
  abstract = {A desirable property of learning systems is to be both effective and interpretable. Towards this goal, recent models have been proposed that first generate an extractive explanation from the input text and then generate a prediction on just the explanation called explain-then-predict models. These models primarily consider the task input as a supervision signal in learning an extractive explanation and do not effectively integrate rationales data as an additional inductive bias to improve task performance. We propose a novel yet simple approach ExPred, which uses multi-task learning in the explanation generation phase effectively trading-off explanation and prediction losses. Next, we use another prediction network on just the extracted explanations for optimizing the task performance. We conduct an extensive evaluation of our approach on three diverse language datasets -- sentiment classification, fact-checking, and question answering -- and find that we substantially outperform existing approaches.},
  booktitle = {Proceedings of the 14th ACM International Conference on Web Search and Data Mining},
  pages = {418–426},
  numpages = {9},
  keywords = {explanation, interpretable by design, learning with rationales, machine learning interpretation, multitask learning},
  location = {Virtual Event, Israel},
  series = {WSDM '21}
}

@inproceedings{Carton:2020,
	title = {Evaluating and Characterizing Human Rationales},
	author = {Carton, Samuel  and
	Rathore, Anirudh  and
	Tan, Chenhao},
	booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
	month = {nov},
	year = {2020},
	address = {Online},
	publisher = {Association for Computational Linguistics},
	url = {https://aclanthology.org/2020.emnlp-main.747},
	doi = {10.18653/v1/2020.emnlp-main.747},
	pages = {9294--9307},
	abstract = {Two main approaches for evaluating the quality of machine-generated rationales are: 1) using human rationales as a gold standard; and 2) automated metrics based on how rationales affect model behavior. An open question, however, is how human rationales fare with these automatic metrics. Analyzing a variety of datasets and models, we find that human rationales do not necessarily perform well on these metrics. To unpack this finding, we propose improved metrics to account for model-dependent baseline performance. We then propose two methods to further characterize rationale quality, one based on model retraining and one on using {``}fidelity curves{''} to reveal properties such as irrelevance and redundancy. Our work leads to actionable suggestions for evaluating and characterizing rationales.},
}

@article{Majumder:2021,
	publtype={informal},
	author={Bodhisattwa Prasad Majumder and Oana-Maria Camburu and Thomas Lukasiewicz and Julian J. McAuley},
	title={Rationale-Inspired Natural Language Explanations with Commonsense},
	year={2021},
	cdate={1609459200000},
	journal={CoRR},
	volume={abs/2106.13876},
	url={https://arxiv.org/abs/2106.13876}
}

@inproceedings{Bosselut:2019,
	title = {{COMET}: Commonsense Transformers for Automatic Knowledge Graph Construction},
	author = {Bosselut, Antoine  and
	Rashkin, Hannah  and
	Sap, Maarten  and
	Malaviya, Chaitanya  and
	Celikyilmaz, Asli  and
	Choi, Yejin},
	booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
	month = jul,
	year = {2019},
	address = {Florence, Italy},
	publisher = {Association for Computational Linguistics},
	url = {https://aclanthology.org/P19-1470},
	doi = {10.18653/v1/P19-1470},
	pages = {4762--4779},
	abstract = {We present the first comprehensive study on automatic knowledge base construction for two prevalent commonsense knowledge graphs: ATOMIC (Sap et al., 2019) and ConceptNet (Speer et al., 2017). Contrary to many conventional KBs that store knowledge with canonical templates, commonsense KBs only store loosely structured open-text descriptions of knowledge. We posit that an important step toward automatic commonsense completion is the development of generative models of commonsense knowledge, and propose COMmonsEnse Transformers (COMET) that learn to generate rich and diverse commonsense descriptions in natural language. Despite the challenges of commonsense modeling, our investigation reveals promising results when implicit knowledge from deep pre-trained language models is transferred to generate explicit knowledge in commonsense knowledge graphs. Empirical results demonstrate that COMET is able to generate novel knowledge that humans rate as high quality, with up to 77.5{\%} (ATOMIC) and 91.7{\%} (ConceptNet) precision at top 1, which approaches human performance for these resources. Our findings suggest that using generative commonsense models for automatic commonsense KB completion could soon be a plausible alternative to extractive methods.},
}

@inproceedings{Lei:2016,
	title = {Rationalizing Neural Predictions},
	author = {Lei, Tao  and
	Barzilay, Regina  and
	Jaakkola, Tommi},
	booktitle = {Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing},
	month = nov,
	year = {2016},
	address = {Austin, Texas},
	publisher = {Association for Computational Linguistics},
	url = {https://aclanthology.org/D16-1011},
	doi = {10.18653/v1/D16-1011},
	pages = {107--117},
}

@inproceedings{dosSantos:2015,
	title = {Learning Hybrid Representations to Retrieve Semantically Equivalent Questions},
	author = {dos Santos, C{\'\i}cero  and
	Barbosa, Luciano  and
	Bogdanova, Dasha  and
	Zadrozny, Bianca},
	booktitle = {Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
	month = jul,
	year = {2015},
	address = {Beijing, China},
	publisher = {Association for Computational Linguistics},
	url = {https://aclanthology.org/P15-2114},
	doi = {10.3115/v1/P15-2114},
	pages = {694--699},
}

@inproceedings{Carton:2022,
	title = {What to Learn, and How: {T}oward Effective Learning from Rationales},
	author = {Carton, Samuel  and
	Kanoria, Surya  and
	Tan, Chenhao},
	booktitle = {Findings of the Association for Computational Linguistics: ACL 2022},
	month = may,
	year = {2022},
	address = {Dublin, Ireland},
	publisher = {Association for Computational Linguistics},
	url = {https://aclanthology.org/2022.findings-acl.86},
	doi = {10.18653/v1/2022.findings-acl.86},
	pages = {1075--1088},
	abstract = {Learning from rationales seeks to augment model prediction accuracy using human-annotated rationales (i.e. subsets of input tokens) that justify their chosen labels, often in the form of intermediate or multitask supervision. While intuitive, this idea has proven elusive in practice. We make two observations about human rationales via empirical analyses:1) maximizing rationale supervision accuracy is not necessarily the optimal objective for improving model accuracy; 2) human rationales vary in whether they provide sufficient information for the model to exploit for prediction.Building on these insights, we propose several novel loss functions and learning strategies, and evaluate their effectiveness on three datasets with human rationales. Our results demonstrate consistent improvements over baselines in both label and rationale accuracy, including a 3{\%} accuracy improvement on MultiRC. Our work highlights the importance of understanding properties of human explanations and exploiting them accordingly in model training.},
}


@inproceedings{Dombrowski:2019,
	title={Explanations can be manipulated and geometry is to blame},
	author={Ann-Kathrin Dombrowski and Maximilian Alber and Christopher J. Anders and Marcel Ackermann and Klaus-Robert M{\"u}ller and Pan Kessel},
	booktitle={NeurIPS},
	year={2019},
	url={https://proceedings.neurips.cc/paper/2019/file/bb836c01cdc9120a9c984c525e4b1a4a-Paper.pdf}
}

@inproceedings{Slack:2020,
	author = {Dylan Slack and Sophie Hilgard and Emily Jia and Sameer Singh and Himabindu Lakkaraju},
	title = {Fooling LIME and SHAP: Adversarial Attacks on Post hoc Explanation Methods},
	booktitle = {AAAI/ACM Conference on AI, Ethics, and Society (AIES)},
	year = {2020},
	url = {https://arxiv.org/pdf/1911.02508.pdf}
}

@inproceedings{Shrikumar:2017,
	author = {Shrikumar, Avanti and Greenside, Peyton and Kundaje, Anshul},
	title = {Learning Important Features through Propagating Activation Differences},
	year = {2017},
	publisher = {JMLR.org},
	abstract = {The purported "black box" nature of neural networks is a barrier to adoption in applications where interpretability is essential. Here we present DeepLIFT (Deep Learning Important FeaTures), a method for decomposing the output prediction of a neural network on a specific input by backpropagating the contributions of all neurons in the network to every feature of the input. DeepLIFT compares the activation of each neuron to its 'reference activation' and assigns contribution scores according to the difference. By optionally giving separate consideration to positive and negative contributions, DeepLIFT can also reveal dependencies which are missed by other approaches. Scores can be computed efficiently in a single backward pass. We apply DeepLIFT to models trained on MNIST and simulated genomic data, and show significant advantages over gradient-based methods. Video tutorial: http://goo.gl/qKb7pL, code: http://goo.gl/RM8jvH.},
	booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
	pages = {3145–3153},
	numpages = {9},
	location = {Sydney, NSW, Australia},
	series = {ICML'17}
}

@inproceedings{Ribeiro:2016,
	title = {{``}Why Should {I} Trust You?{''}: Explaining the Predictions of Any Classifier},
	author = {Ribeiro, Marco  and
	Singh, Sameer  and
	Guestrin, Carlos},
	booktitle = {Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Demonstrations},
	month = jun,
	year = {2016},
	address = {San Diego, California},
	publisher = {Association for Computational Linguistics},
	url = {https://aclanthology.org/N16-3020},
	doi = {10.18653/v1/N16-3020},
	pages = {97--101},
}

@inproceedings{Lundberg:2017,
	author = {Lundberg, Scott M and Lee, Su-In},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
	pages = {},
	publisher = {Curran Associates, Inc.},
	title = {A Unified Approach to Interpreting Model Predictions},
	url = {https://proceedings.neurips.cc/paper/2017/file/8a20a8621978632d76c43dfd28b67767-Paper.pdf},
	volume = {30},
	year = {2017}
}

@article{Lipton:2018,
	author = {Lipton, Zachary C.},
	title = {The Mythos of Model Interpretability: In Machine Learning, the Concept of Interpretability is Both Important and Slippery.},
	year = {2018},
	issue_date = {May-June 2018},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {16},
	number = {3},
	issn = {1542-7730},
	url = {https://doi.org/10.1145/3236386.3241340},
	doi = {10.1145/3236386.3241340},
	abstract = {Supervised machine-learning models boast remarkable predictive capabilities. But can you trust your model? Will it work in deployment? What else can it tell you about the world?},
	journal = {Queue},
	month = {jun},
	pages = {31–57},
	numpages = {27}
}

@misc{Krishna:2022,
	doi = {10.48550/ARXIV.2202.01602},
	url = {https://arxiv.org/abs/2202.01602},
	author = {Krishna, Satyapriya and Han, Tessa and Gu, Alex and Pombra, Javin and Jabbari, Shahin and Wu, Steven and Lakkaraju, Himabindu},
	keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
	title = {The Disagreement Problem in Explainable Machine Learning: A Practitioner's Perspective},
	publisher = {arXiv},
	year = {2022},
	copyright = {Creative Commons Attribution 4.0 International}
}


