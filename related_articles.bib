@inproceedings{DeYoung:2020,
    title = {{ERASER}: {A} Benchmark to Evaluate Rationalized {NLP} Models},
    author = {DeYoung, Jay  and
      Jain, Sarthak  and
      Rajani, Nazneen Fatema  and
      Lehman, Eric  and
      Xiong, Caiming  and
      Socher, Richard  and
      Wallace, Byron C.},
    booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
    month = {jul},
    year = {2020},
    address = {Online},
    publisher = {Association for Computational Linguistics},
    url = {https://aclanthology.org/2020.acl-main.408},
    doi = {10.18653/v1/2020.acl-main.408},
    pages = {4443--4458},
    abstract = {State-of-the-art models in NLP are now predominantly based on deep neural networks that are opaque in terms of how they come to make predictions. This limitation has increased interest in designing more interpretable deep models for NLP that reveal the {`}reasoning{'} behind model outputs. But work in this direction has been conducted on different datasets and tasks with correspondingly unique aims and metrics; this makes it difficult to track progress. We propose the \textbf{E}valuating \textbf{R}ationales \textbf{A}nd \textbf{S}imple \textbf{E}nglish \textbf{R}easoning (\textbf{ERASER} a benchmark to advance research on interpretable models in NLP. This benchmark comprises multiple datasets and tasks for which human annotations of {``}rationales{''} (supporting evidence) have been collected. We propose several metrics that aim to capture how well the rationales provided by models align with human rationales, and also how \textit{faithful} these rationales are (i.e., the degree to which provided rationales influenced the corresponding predictions). Our hope is that releasing this benchmark facilitates progress on designing more interpretable NLP systems. The benchmark, code, and documentation are available at \url{https://www.eraserbenchmark.com/}},
}

@inproceedings{Mathew:2021,
	title={HateXplain: A Benchmark Dataset for Explainable Hate Speech Detection},
	author={Mathew, Binny and Saha, Punyajoy and Yimam, Seid Muhie and Biemann, Chris and Goyal, Pawan and Mukherjee, Animesh},
	booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
	volume={35},
	number={17},
	pages={14867--14875},
	year={2021}
}

@inproceedings{Thorne:2018,
	title = {{FEVER}: a Large-scale Dataset for Fact Extraction and {VER}ification},
	author = {Thorne, James  and
	Vlachos, Andreas  and
	Christodoulopoulos, Christos  and
	Mittal, Arpit},
	booktitle = {Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)},
	month = {jun},
	year = {2018},
	address = {New Orleans, Louisiana},
	publisher = {Association for Computational Linguistics},
	url = {https://aclanthology.org/N18-1074},
	doi = {10.18653/v1/N18-1074},
	pages = {809--819},
	abstract = {In this paper we introduce a new publicly available dataset for verification against textual sources, FEVER: Fact Extraction and VERification. It consists of 185,445 claims generated by altering sentences extracted from Wikipedia and subsequently verified without knowledge of the sentence they were derived from. The claims are classified as Supported, Refuted or NotEnoughInfo by annotators achieving 0.6841 in Fleiss kappa. For the first two classes, the annotators also recorded the sentence(s) forming the necessary evidence for their judgment. To characterize the challenge of the dataset presented, we develop a pipeline approach and compare it to suitably designed oracles. The best accuracy we achieve on labeling a claim accompanied by the correct evidence is 31.87{\%}, while if we ignore the evidence we achieve 50.91{\%}. Thus we believe that FEVER is a challenging testbed that will help stimulate progress on claim verification against textual sources.},
}

@inproceedings{Han:2020,
    title = {Explaining Black Box Predictions and Unveiling Data Artifacts through Influence Functions},
    author = {Han, Xiaochuang  and
      Wallace, Byron C.  and
      Tsvetkov, Yulia},
    booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
    month = {jul},
    year = {2020},
    address = {Online},
    publisher = {Association for Computational Linguistics},
    url = {https://aclanthology.org/2020.acl-main.492},
    doi = {10.18653/v1/2020.acl-main.492},
    pages = {5553--5563},
    abstract = {Modern deep learning models for NLP are notoriously opaque. This has motivated the development of methods for interpreting such models, e.g., via gradient-based saliency maps or the visualization of attention weights. Such approaches aim to provide explanations for a particular model prediction by highlighting important words in the corresponding input text. While this might be useful for tasks where decisions are explicitly influenced by individual tokens in the input, we suspect that such highlighting is not suitable for tasks where model decisions should be driven by more complex reasoning. In this work, we investigate the use of influence functions for NLP, providing an alternative approach to interpreting neural text classifiers. Influence functions explain the decisions of a model by identifying influential training examples. Despite the promise of this approach, influence functions have not yet been extensively evaluated in the context of NLP, a gap addressed by this work. We conduct a comparison between influence functions and common word-saliency methods on representative tasks. As suspected, we find that influence functions are particularly useful for natural language inference, a task in which {`}saliency maps{'} may not have clear interpretation. Furthermore, we develop a new quantitative measure based on influence functions that can reveal artifacts in training data.},
}

@InProceedings{Ruiter:2022,
  author    = {Ruiter, Dana  and  Reiners, Liane and Geet D'Sa, Ashwin and Kleinbauer, Thomas and Fohr, Dominique and Illina, Irina and Klakow, Dietrich and Schemer, Christian and Monnier, Angeliki},
  title     = {Placing M-Phasis on the Plurality of Hate: A Feature-Based Corpus of Hate Online},
  booktitle      = {Proceedings of The 14th Language Resources and Evaluation Conference},
  month          = {June},
  year           = {2022},
  address        = {Marseille, France},
  publisher      = {European Language Resources Association},
  url            = {https://arxiv.org/pdf/2204.13400.pdf}
}

@inproceedings{Zhang:2021,
  author = {Zhang, Zijian and Rudra, Koustav and Anand, Avishek},
  title = {Explain and Predict, and Then Predict Again},
  year = {2021},
  isbn = {9781450382977},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3437963.3441758},
  doi = {10.1145/3437963.3441758},
  abstract = {A desirable property of learning systems is to be both effective and interpretable. Towards this goal, recent models have been proposed that first generate an extractive explanation from the input text and then generate a prediction on just the explanation called explain-then-predict models. These models primarily consider the task input as a supervision signal in learning an extractive explanation and do not effectively integrate rationales data as an additional inductive bias to improve task performance. We propose a novel yet simple approach ExPred, which uses multi-task learning in the explanation generation phase effectively trading-off explanation and prediction losses. Next, we use another prediction network on just the extracted explanations for optimizing the task performance. We conduct an extensive evaluation of our approach on three diverse language datasets -- sentiment classification, fact-checking, and question answering -- and find that we substantially outperform existing approaches.},
  booktitle = {Proceedings of the 14th ACM International Conference on Web Search and Data Mining},
  pages = {418â€“426},
  numpages = {9},
  keywords = {explanation, interpretable by design, learning with rationales, machine learning interpretation, multitask learning},
  location = {Virtual Event, Israel},
  series = {WSDM '21}
}

@inproceedings{Carton:2020,
	title = {Evaluating and Characterizing Human Rationales},
	author = {Carton, Samuel  and
	Rathore, Anirudh  and
	Tan, Chenhao},
	booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
	month = {nov},
	year = {2020},
	address = {Online},
	publisher = {Association for Computational Linguistics},
	url = {https://aclanthology.org/2020.emnlp-main.747},
	doi = {10.18653/v1/2020.emnlp-main.747},
	pages = {9294--9307},
	abstract = {Two main approaches for evaluating the quality of machine-generated rationales are: 1) using human rationales as a gold standard; and 2) automated metrics based on how rationales affect model behavior. An open question, however, is how human rationales fare with these automatic metrics. Analyzing a variety of datasets and models, we find that human rationales do not necessarily perform well on these metrics. To unpack this finding, we propose improved metrics to account for model-dependent baseline performance. We then propose two methods to further characterize rationale quality, one based on model retraining and one on using {``}fidelity curves{''} to reveal properties such as irrelevance and redundancy. Our work leads to actionable suggestions for evaluating and characterizing rationales.},
}

@article{Majumder:2021,
	publtype={informal},
	author={Bodhisattwa Prasad Majumder and Oana-Maria Camburu and Thomas Lukasiewicz and Julian J. McAuley},
	title={Rationale-Inspired Natural Language Explanations with Commonsense},
	year={2021},
	cdate={1609459200000},
	journal={CoRR},
	volume={abs/2106.13876},
	url={https://arxiv.org/abs/2106.13876}
}

